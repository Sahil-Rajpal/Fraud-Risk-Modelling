# ðŸ“Œ Credit Risk Modeling

## ðŸŽ¯ Project Objective  

The goal of this project is to build a **machine learning model** to predict fraudulent credit card transactions. By analyzing transaction and customer data, the model helps stakeholders:  

- Detect fraudulent patterns in real-time  
- Minimize financial losses due to fraud  
- Improve trust and security in digital transactions  
- Optimize fraud prevention strategies for financial institutions  

## ðŸ“‚ Dataset

- **Source:** [Credit Card Fraud Detection Dataset (Kaggle)](https://www.kaggle.com/datasets)   
- **Duration Covered:** 1st Jan 2019 â€“ 31st Dec 2020  
- **Scale:**  
  - 1000 customers  
  - 800 merchants  
  - Transactions across multiple categories (e.g., food, travel, shopping, personal care, entertainment, etc.)  
- **Files:**  
  - `fraudTrain.csv` â€“ Training dataset  
  - `fraudTest.csv` â€“ Testing dataset  
- **Features:** 46 columns including:  
  - Transaction details (`trans_date_trans_time`, `cc_num`, `merchant`, `category`, `amt`)  
  - Customer details (`first`, `last`, `gender`, `street`)  
  - Target label: `is_fraud` (0 = Legitimate, 1 = Fraudulent)  

The dataset provides a realistic representation of both legitimate and fraudulent transactions, enabling the development of **fraud detection models** that can identify abnormal spending patterns and reduce financial risks. 


## ðŸŽ¯ Business Objectives  

The primary objective of this project is to **assess credit risk** by predicting the likelihood of a customer defaulting on their credit obligations.  

### Specific Goals:  
- ðŸ“Š **Data Exploration & Preprocessing** â€“ Understand data distribution, handle missing values, outliers, and perform feature engineering.  
- ðŸ¤– **Model Development** â€“ Train and evaluate multiple machine learning models (Decision Trees , Random Forest, XGBoost, etc.) to classify borrowers as *low-risk* or *high-risk*.  
- ðŸ“ˆ **Performance Evaluation** â€“ Compare models using metrics like Accuracy, Precision, Recall, F1-score, AUC-ROC and KS Metrics to identify the best-performing model.  
- ðŸ”Ž **Interpretability** â€“ Use feature importance and Woe-Iv values to explain which factors contribute most to Fraud risk.  
- ðŸ¦ **Business Impact** â€“ Provide a reliable tool for financial institutions to minimize risk of fruadlent transactions.

## ðŸ“‚ Data Description  

The project uses the **fraudTrain.csv** and **fraudTest.csv** dataset, which contains detailed information about credit card transactions.  
Below are the key fields:  

- **Unnamed: 0** â†’ Unique ID for each transaction  
- **trans_date_trans_time** â†’ Date and time of the transaction  
- **cc_num** â†’ Credit card number  
- **merchant** â†’ Merchant name where the transaction occurred  
- **category** â†’ Transaction category (e.g., personal_care, health_fitness, travel, misc_pos, etc.)  
- **first** â†’ First name of the cardholder  
- **last** â†’ Last name of the cardholder  
- **gender** â†’ Gender of the cardholder (M/F)  
- **amt** â†’ Transaction amount (value spent)  
- **street** â†’ Street address of the transaction location  
- **city** â†’ City where the transaction occurred (200+ cities)  
- **state** â†’ State where the transaction occurred  
- **zip** â†’ Zip or pincode of the transaction region  
- **lat, long** â†’ Latitude and longitude of the cardholderâ€™s location  
- **city_pop** â†’ Population of the city (e.g., 10,000; 23,000, etc.)  
- **job** â†’ Occupation of the cardholder  
- **dob** â†’ Date of birth of the cardholder  
- **trans_num** â†’ Unique transaction number generated for each transaction  
- **merch_lat, merch_long** â†’ Latitude and longitude of the merchant/business location  
- **is_fraud** â†’ Target variable (0 = Legitimate, 1 = Fraudulent)  


## ðŸ”Ž Exploratory Data Analysis (EDA)  

A detailed exploratory data analysis (EDA) was performed to understand the dataset, remove irrelevant features, and identify patterns useful for fraud detection.  

### Feature-Wise Insights  

- **Unnamed: 0** â†’ Contains 555,719 counts; only an index column, hence **dropped**.  
- **trans_date_trans_time** â†’ Date & time of transaction; used for time-based analysis but not directly for modeling.  
- **cc_num** â†’ Unique card number; not used in modeling but helpful for calculating **total number of transactions** and **transaction volume per card**.  
- **merchant** â†’ Merchant names; removed due to high cardinality and irrelevance for modeling.  
- **category** â†’ Transaction category (e.g., *personal_care, health_fitness, travel, misc_pos*).  
  - Categories were **grouped into 4â€“5 bins** based on fraud transaction counts and relevance.  
- **first, last** â†’ First/last names of cardholders; **removed** as irrelevant.  
- **gender** â†’ Gender distribution of transactions shows:  
  - Female: **54.86%**  
  - Male: **45.13%**  
  - Fraud rate varies slightly between genders, making it relevant for modeling.  
- **amt** â†’ Transaction amount; critical for modeling since fraud may depend on transaction value (small or large).  
- **street** â†’ Street address; **dropped** due to irrelevance.  
- **city, state** â†’ Geographic information. Certain states show higher fraud rates. Example:  

| State | Total Txn | Fraud Txn | Fraud Rate |
|-------|-----------|-----------|------------|
| NY    | 35,918    | 175       | 0.49       |
| IN    | 11,959    | 75        | 0.63       |
| VA    | 12,506    | 75        | 0.60       |
| OR    | 7,811     | 48        | 0.61       |
| MS    | 8,833     | 54        | 0.61       |

- **zip** â†’ High cardinality; not useful for modeling.  
- **lat, long** â†’ Latitude & longitude of cardholder; potential for fraud hotspot analysis.  
- **city_pop** â†’ Population of the cardholderâ€™s city; can help detect fraud trends in **small vs. large cities**.  
- **job** â†’ Occupation of the cardholder; may provide socioeconomic insight into fraud likelihood.  
- **dob** â†’ Date of birth; used to derive **age feature**, which can influence fraud risk.  
- **trans_num** â†’ Unique transaction ID; dropped from modeling.   


âœ… After EDA, irrelevant columns (e.g., *Unnamed: 0, first, last, merchant, street, zip, trans_num etc) were removed, while meaningful features were **transformed or engineered** for modeling.  

  ## ðŸ”§ Feature Engineering for Credit Card Fraud Detection

---

## ðŸ“Œ 1. Dropped / Irrelevant Columns
The following columns were removed as they donâ€™t contribute directly to modeling:
- `Unnamed: 0` â†’ index column  
- `cc_num` â†’ card number (used only for aggregation, not modeling)  
- `merchant` â†’ merchant name (irrelevant for fraud prediction)  
- `street`, `zip`, `city` â†’ highly granular, less predictive  
- `first`, `last` â†’ irrelevant personal identifiers  
- `trans_num` â†’ unique transaction ID  

---

## ðŸ“Œ 2. Feature Engineering Steps

### ðŸ—“ï¸ 2.1 Date of Birth â†’ Age
- Converted `dob` into `age`.  
- Created **age bins**:  
  - `0â€“18`, `19â€“30`, `31â€“45`, `46â€“60`, `61â€“75`, `76+`  
- Applied **One-Hot Encoding (OHE)** to bins.  
- Dropped original `dob` column.  

---

### ðŸš» 2.2 Gender
- Encoded gender (`M`, `F`) using **OHE**.  

---

### ðŸ›’ 2.3 Transaction Category
- Original categories (14 unique) grouped into **4 broader classes**:  

| Original Categories | Grouped Category |
|---------------------|------------------|
| `grocery_pos`, `grocery_net`, `gas_transport`, `home` | Essentials |
| `personal_care`, `health_fitness`, `kids_pets` | Lifestyle & Wellbeing |
| `entertainment`, `food_dining`, `travel` | Discretionary |
| `shopping_pos`, `shopping_net`, `misc_pos`, `misc_net` | Shopping & Misc |

- Applied **OHE** on grouped categories.  

---

### ðŸŒ 2.4 Location Features
- Applied **reverse geocoding** on (`lat`, `long`) â†’ extracted **State, City, Country**.  
- Dropped `City` and `Country`.  
- Engineered **State â†’ Region mapping**:

| Region     | Example States |
|------------|----------------|
| Western    | California, Oregon, Washington, Nevada, Arizona |
| Northern   | Minnesota, Iowa, Illinois, Michigan, Ohio |
| Southern   | Texas, Florida, Georgia, Virginia, Alabama |
| Eastern    | New York, New Jersey, Pennsylvania, Massachusetts |

- Applied **OHE** on `Region`.  

---

### ðŸ™ï¸ 2.5 City Population
- Population (`city_pop`) was highly skewed.  
- Created **population bins**:  
  - `Micro Town` â†’ 0â€“25,000  
  - `Small Town` â†’ 25,001â€“50,000  
  - `Mid City` â†’ 50,001â€“100,000  
  - `Large City` â†’ 100,001â€“200,000  
  - `Metro City` â†’ 200,001+  

- Example fraud distribution by city category:  

| City Category | Total Txn | Fraud Txn | Fraud Rate |
|---------------|-----------|-----------|------------|
| Micro Town    | 426,735   | 1,757     | ~0%        |
| Small Town    | 25,467    | 44        | ~0%        |
| Mid City      | 27,601    | 136       | ~0%        |
| Large City    | 24,501    | 46        | ~0%        |
| Metro City    | 51,415    | 162       | ~0%        |

- Applied **OHE** on city categories.  

---

### ðŸ’³ 2.6 Transaction Amount
- Retained `amt` (transaction amount) as-is.  
- Hypothesis: Fraud may occur more often at **smaller amounts** (to avoid detection) or at **sudden high-value transactions**.  

---

### ðŸ•’ 2.7 Transaction Time
- From `trans_date_trans_time`, derived additional temporal features:  
  - `hour` of transaction  
  - `day_of_week`  
  - `month`  
- Useful to capture fraud trends (e.g., late-night or weekend fraud).  

---

## âœ… Final Engineered Feature Set
1. **Demographics**  
   - `age_bin` (OHE)  
   - `gender` (OHE)  

2. **Transaction Info**  
   - `amt`  
   - `category_grouped` (OHE)  
   - `trans_time_features` (`hour`, `day_of_week`, `month`)  

3. **Geographic Info**  
   - `region` (OHE)  
   - `city_category` (OHE)  

---
# ðŸŽ¯ Feature Selection  

After feature engineering, the dataset contained a large number of variables (including multiple one-hot encoded columns). To improve model performance, reduce noise, and avoid overfitting, **feature selection** techniques were applied.  

---

## ðŸ“Œ Weight of Evidence (WOE)

The **Weight of Evidence (WOE)** is used to measure how well an independent variable separates the binary target classes (e.g., fraud vs. non-fraud).  
It transforms categorical or continuous predictors into a scale that reflects their predictive power.  

**Interpretation:**
- WOE > 0 â†’ Higher likelihood of being a non-event (e.g., non-fraud).  
- WOE < 0 â†’ Higher likelihood of being an event (e.g., fraud).  
- WOE = 0 â†’ No predictive power.  

**Formula:**  

`WOE = ln( (% of non-events) / (% of events) )`  


---

## ðŸ“Œ Information Value (IV)

The **Information Value (IV)** quantifies the predictive strength of a variable in relation to the target.  
It is derived from the distribution differences between events and non-events.  

**Interpretation (commonly used thresholds):**
- IV < 0.02 â†’ Not predictive  
- 0.02 â‰¤ IV < 0.1 â†’ Weak predictor  
- 0.1 â‰¤ IV < 0.3 â†’ Medium predictor  
- 0.3 â‰¤ IV < 0.5 â†’ Strong predictor  
- IV â‰¥ 0.5 â†’ Suspiciously powerful (possible data leakage)  

**Formula:**  

`IV = Î£ [ ( % of non-events - % of events ) * WOE ]`  


---

## ðŸ“Š IV Values of Features  
The following table summarizes the **calculated IV values** (from Excel results):  

| Feature                              | IV Value       |
|--------------------------------------|----------------|
| amt                                  | 3.8378         |
| category_bin_Shopping & Misc         | 0.2049         |
| category_bin_Lifestyle & Wellbeing   | 0.1531         |
| city_category_Small Town             | 0.0211         |
| city_category_Micro Town             | 0.0162         |
| age_group_61-75                      | 0.0093         |
| age_group_76+                        | 0.0058         |
| age_group_46-60                      | 0.0054         |
| city_category_Metro City             | 0.0038         |
| city_category_Mid City               | 0.0036         |
| category_bin_Essentials              | 0.0024         |
| region_Northern                      | 0.0017         |
| region_Southern                      | 0.0015         |
| age_group_31-45                      | 0.0015         |
| gender_M                             | 0.0001         |
| region_Western                       | 0.00006        |

---

## âš–ï¸ Feature Selection Approaches  

### âœ… Approach 1: High-IV Features Only  
- Use only features with **high IV values** (e.g., `amt`, `category_bin_Shopping & Misc`, `category_bin_Lifestyle & Wellbeing`).  
- Pros: Reduces dimensionality, highlights the strongest predictors.  
- Cons: Risk of oversimplifying â†’ potential **overfitting** since multivariate effects are ignored.  

### âœ… Approach 2: Keep All Features  
- Use all engineered features, even those with **low IV values**, since in multivariate analysis their **combinations** can still contribute to detecting fraud.  
- Pros: Richer feature space for models like Random Forest, XGBoost.  
- Cons: Higher complexity, potential noise introduction.  

---

## ðŸŽ¯ Final Choice  
We proceeded with **Approach 2 (All Engineered Features)** since fraud detection is often driven by **combinations of variables** rather than single predictors.  

Even though some features have low IV values, they may still provide **useful multivariate interactions** (e.g., transaction amount Ã— region, age group Ã— category).  
Tree-based models like **Random Forest** and **XGBoost** can effectively capture these relationships.  

ðŸ“Œ This approach ensures that the model has a **richer feature space** and can make more accurate splits, leading to better fraud detection performance in practice.

---

## âš–ï¸ Handling Imbalanced Data  

Our dataset is highly imbalanced:  
- **Class 0 (Non-Fraud):** 553,574 (~99.62%)  
- **Class 1 (Fraud):** 2,145 (~0.38%)  
- **Total:** 555,719 rows  

This imbalance can cause issues like:  
- Model bias toward the majority class  
- Unreliable evaluation metrics  

To address this, we applied **sampling and weighting techniques**:



### ðŸ”¹ 1. SMOTE (Synthetic Minority Oversampling Technique)  
- Creates **synthetic samples** for the minority class (instead of just duplicating).  
- âœ… Pros: Better class balance, reduced bias, prevents overfitting.  
- âŒ Cons: Computationally heavy, may amplify noise, struggles with categorical/high-dimensional data.  



### ðŸ”¹ 2. ADASYN (Adaptive Synthetic Sampling)  
- Extension of SMOTE that generates **more synthetic samples** for â€œhard-to-learnâ€ cases (low density or near decision boundary).  
- âœ… Pros: Focuses on difficult examples, improves discrimination between classes.  
- âŒ Cons: More complex, sensitive to noise, risk of overfitting.  



### ðŸ”¹ 3. Cost-Sensitive Learning  
- Modifies the **loss function** by assigning **higher weights** to the minority class and lower weights to the majority class.  
- âœ… Pros: Better performance on imbalanced data, useful in high-stakes domains (finance, healthcare).  
- âŒ Cons: Requires domain knowledge to set costs, adds complexity, can be computationally intensive.  



---

## ðŸ¤– Model Training  

We experimented with multiple models to detect credit card fraud:  
- **Decision Tree**  
- **Random Forest**  
- **XGBoost**  
- **LightGBM**  

### ðŸ”§ Training Approach  
- Built **baseline models** for comparison.  
- Applied **hyperparameter tuning** using **GridSearchCV**.  
- Incorporated **sampling techniques**:  
  - **SMOTE (oversampling)**  
  - **Cost-Sensitive Learning (class weighting)**  

---

### ðŸ† Best Model  
The best results were obtained using a **combination of Cost-Sensitive Learning + XGBoost**.  

**Performance Metrics:**  
- âœ… **Accuracy:** 0.96  
- âœ… **Macro Avg Recall:** 0.93
- âœ… **AUC Score:** 0.98  
- âœ… **KS Metric Difference (Train vs Test):** 3.58  

**Fraud Detection Results:**  
- Detected **579 fraud transactions** out of **644** in test data.  
- Successfully detected **159,000+ legitimate transactions** out of **166,000+** in test data.  

ðŸ“Œ This combination offered the **best balance of accuracy, recall, and generalization**.  


